import os
import aigco
from transformers import AutoTokenizer
from huggingface_hub import snapshot_download  # <--- æ–°å¢žå¯¼å…¥
from dotenv import load_dotenv

load_dotenv()

# æ¨¡åž‹åœ¨ HF ä¸Šçš„ ID
REPO_ID = "Qwen/Qwen3-0.6B"

logger = aigco.logger(name="qwen3_inference")


def main():
    # è‡ªåŠ¨èŽ·å–ç¼“å­˜ä¸­çš„çœŸå®žç»å¯¹è·¯å¾„
    try:
        # local_files_only=True ç¡®ä¿å®ƒåªä»Žæœ¬åœ°æ‰¾ï¼Œä¸ä¼šåŽ»è”ç½‘ä¸‹è½½
        model_path = snapshot_download(repo_id=REPO_ID, local_files_only=True)
        print(f"ðŸ“ æ‰¾åˆ°æ¨¡åž‹è·¯å¾„: {model_path}")
    except Exception as e:
        print(f"âŒ æ— æ³•åœ¨ç¼“å­˜ä¸­æ‰¾åˆ°æ¨¡åž‹ {REPO_ID}ï¼Œè¯·ç¡®è®¤æ˜¯å¦å·²ä¸‹è½½ã€‚")
        return

    # ä½¿ç”¨è‡ªåŠ¨èŽ·å–çš„è·¯å¾„
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    llm = aigco.inference.LLM(model_path, enforce_eager=True, tensor_parallel_size=1)

    sampling_params = aigco.inference.SamplingParams(temperature=0.6, max_tokens=256)
    prompts = ["introduce yourself", "list all prime numbers within 100"]

    prompts = [
        tokenizer.apply_chat_template(
            [{"role": "user", "content": prompt}],
            tokenize=False,
            add_generation_prompt=True,
        )
        for prompt in prompts
    ]

    outputs = llm.generate(prompts, sampling_params)

    for prompt, output in zip(prompts, outputs):
        print(f"\nPrompt: {prompt!r}\nCompletion: {output['text']!r}")


if __name__ == "__main__":
    main()
